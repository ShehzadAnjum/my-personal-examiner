"""
AttemptedQuestion Model

Represents a student's answer to a specific question within an exam attempt.

Phase II Foundation:
- Links attempts to questions (many-to-one relationship)
- Stores student answers and marking feedback
- Tracks marks awarded and weaknesses

Phase III Enhancement (T017):
- confidence_score: 0-100 scale from 6-signal heuristic
- needs_review: Boolean flag (<70% confidence triggers manual review)
- reviewed_by: Human examiner who reviewed low-confidence marks
- reviewed_at: Timestamp of manual review
"""

from datetime import datetime, timezone
from typing import Any, Optional
from uuid import UUID, uuid4

from sqlalchemy import JSON, Column
from sqlmodel import Field, SQLModel


class AttemptedQuestion(SQLModel, table=True):
    """
    AttemptedQuestion entity

    Represents a student's answer to a specific question within an exam attempt.
    One attempt can have multiple attempted_questions (one per question in exam).

    Attributes:
        id: Unique attempted question identifier (UUID primary key)
        attempt_id: Foreign key to attempt (CASCADE DELETE)
        question_id: Foreign key to question (CASCADE DELETE)
        student_answer: Student's answer text (null if unanswered)
        marks_awarded: Marks awarded by Marker Agent (null until marked)
        marking_feedback: JSONB object with errors[], strengths[], suggestions[]
        weaknesses: JSONB array of weakness categories (e.g., ["AO1: Definition", "AO3: Evaluation"])
        model_answer: A* exemplar answer generated by Reviewer Agent
        confidence_score: Confidence in marking accuracy (0-100, from 6-signal heuristic)
        needs_review: Flag for manual review queue (<70% confidence)
        reviewed_by: Human examiner who reviewed (null if not reviewed)
        reviewed_at: Timestamp of manual review (null if not reviewed)
        created_at: Timestamp when record was created
        updated_at: Timestamp when record was last modified

    Examples:
        >>> aq = AttemptedQuestion(
        ...     attempt_id=attempt_uuid,
        ...     question_id=question_uuid,
        ...     student_answer="Supply and demand determine prices...",
        ...     marks_awarded=8,
        ...     confidence_score=85,
        ...     needs_review=False,
        ...     marking_feedback={"errors": [...], "strengths": [...]}
        ... )
        >>> # Low confidence mark requiring review
        >>> aq_low = AttemptedQuestion(
        ...     attempt_id=attempt_uuid,
        ...     question_id=question_uuid,
        ...     student_answer="Short answer with vague terms",
        ...     marks_awarded=5,
        ...     confidence_score=62,
        ...     needs_review=True  # <70% threshold
        ... )

    Constitutional Compliance:
        - Multi-tenant isolation: attempt_id links to student (Principle V)
        - A* standard marking: confidence scoring ensures quality (Principle II)
        - Constructive feedback: marking_feedback explains errors (Principle VI)
    """

    __tablename__ = "attempted_questions"

    # Primary Key
    id: UUID = Field(
        default_factory=uuid4,
        primary_key=True,
        nullable=False,
    )

    # Foreign Keys
    attempt_id: UUID = Field(
        foreign_key="attempts.id",
        nullable=False,
        index=True,
        description="Attempt ID (CASCADE DELETE)",
    )

    question_id: UUID = Field(
        foreign_key="questions.id",
        nullable=False,
        index=True,
        description="Question ID (CASCADE DELETE)",
    )

    # Student Response
    student_answer: Optional[str] = Field(
        default=None,
        description="Student's answer text (null if unanswered)",
    )

    # Marking Results
    marks_awarded: Optional[int] = Field(
        default=None,
        ge=0,
        description="Marks awarded by Marker Agent (null until marked)",
    )

    # Feedback (JSONB for flexibility)
    marking_feedback: Optional[dict[str, Any]] = Field(
        default=None,
        sa_column=Column(JSON),
        description="JSONB object with errors[], strengths[], suggestions[]",
    )

    weaknesses: Optional[dict[str, Any]] = Field(
        default=None,
        sa_column=Column(JSON),
        description="JSONB array of weakness categories",
    )

    model_answer: Optional[str] = Field(
        default=None,
        description="A* exemplar answer generated by Reviewer Agent",
    )

    # Phase III: Confidence Scoring (T017)
    confidence_score: Optional[int] = Field(
        default=None,
        ge=0,
        le=100,
        description="Confidence in marking accuracy (0-100, from 6-signal heuristic)",
    )

    needs_review: bool = Field(
        default=False,
        nullable=False,
        description="Flag for manual review queue (true if confidence < 70%)",
    )

    reviewed_by: Optional[UUID] = Field(
        default=None,
        foreign_key="students.id",
        description="Human examiner who reviewed low-confidence mark (null if not reviewed)",
    )

    reviewed_at: Optional[datetime] = Field(
        default=None,
        description="Timestamp of manual review (null if not reviewed)",
    )

    # Timestamps
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        nullable=False,
        description="Timestamp when record was created",
    )

    updated_at: Optional[datetime] = Field(
        default=None,
        description="Timestamp when record was last modified",
    )

    # Validation methods
    def is_answered(self) -> bool:
        """
        Check if student provided an answer

        Returns:
            bool: True if student_answer is not None
        """
        return self.student_answer is not None

    def is_marked(self) -> bool:
        """
        Check if question has been marked

        Returns:
            bool: True if marks_awarded is not None
        """
        return self.marks_awarded is not None

    def requires_manual_review(self) -> bool:
        """
        Check if marking requires manual review

        Returns:
            bool: True if confidence_score < 70% or needs_review is True
        """
        if self.confidence_score is None:
            return False
        return self.confidence_score < 70 or self.needs_review

    def is_reviewed(self) -> bool:
        """
        Check if question has been manually reviewed

        Returns:
            bool: True if reviewed_by and reviewed_at are not None
        """
        return self.reviewed_by is not None and self.reviewed_at is not None

    def __repr__(self) -> str:
        """String representation for debugging"""
        return (
            f"<AttemptedQuestion(attempt={str(self.attempt_id)[:8]}, "
            f"question={str(self.question_id)[:8]}, marks={self.marks_awarded}, "
            f"confidence={self.confidence_score}, needs_review={self.needs_review})>"
        )
